# Asakusa on Spark - Example Batch Application

This project contains an example batch application working with [Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark) and Asakusa on MapReduce.

## How to build

### Requirements

* Java SE Development Kit >= 1.7

### Build with Gradle

You can build exmaple application using the project included Gradle Wrapper.

On Unix-like platforms such as Linux and Mac OS X:

```
./gradlew clean assemble
```

On Windows:

```
gradlew.bat clean assemble
```

## How to run tests

Before you can run batch application tests, you need to install Asakusa Framewrok runtime environment on your machine.

The first you need to set `ASAKUSA_HOME` environment variable to Asakusa Framework install directory path.

```
export ASAKUSA_HOME=$HOME/asakusafw
```

And you can install with `installAsakusafw` gradle task. This installs Asakusa Framework runtime modules to `ASAKUSA_HOME`.

```
./gradlew installAsakusafw
```

Then you can run any tests.

```
./gradlew test
```

## How to deploy

A deployment archive file `build/example-basic-spark.tar.gz` is generated by run `./gradlew assemble`.
You can put it on your target machine and extract under `ASAKUSA_HOME` directory.

```
mkdir -p "$ASAKUSA_HOME"
cd "$ASAKUSA_HOME"
tar -xf /path/to/example-basic-spark.tar.gz
find "$ASAKUSA_HOME" -name "*.sh" | xargs chmod u+x
```

## How to run application

### Supported platforms

[Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark) is currently tested on some Linux platforms (CentOS, Ubuntu, Amazon Linux).

### Requirements

* Java SE Development Kit >= 1.7
* [Apache Spark](http://spark.apache.org/) 1.6.1
* [Apache Hadoop](http://hadoop.apache.org/) ( strongly recommended on YARN environment )

Any batch applications using [Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark) need not only Spark runtime environment but also Hadoop MapReduce runtime environment because writing final output data will be executed on Hadoop MapReduce job.

### Setting sample data

This example application reads input data form `$HOME/target/testing/direcito` by default.
You can use sample data includes `$ASAKUSA_HOME/example-dataset`.

```
mkdir -p "$HOME/target/testing/directio"
cp -a $ASAKUSA_HOME/example-dataset/* $HOME/target/testing/directio
```

### Setting environment variables

You need to set `SPARK_CMD` environment variables to `spark-submit` command path of using Spark environment, or set path to `<path/to/spark>/bin` in your `PATH` environment variables.

```
export SPARK_CMD=/opt/spark/bin/spark-submit
```

### Running application

Now you can run application by the following command.

```
$ASAKUSA_HOME/yaess/bin/yaess-batch.sh spark.example.summarizeSales -A date=2011-04-01
```

Then you should see the output files on `$HOME/target/testing/directio/result` directory.

* `category/result.csv`
* `error/${date}.csv`

## Referred Projects
* [Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark)

## Resources
* [Asakusa on Spark Documentation (ja)](http://docs.asakusafw.com/asakusa-on-spark/)
