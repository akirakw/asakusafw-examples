# Asakusa on Spark - Example Batch Application

This project contains an example batch application working with [Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark).

## How to build

### Requirements

* Java SE Development Kit >= 1.8

### Build with Gradle

You can build exmaple application using the project included Gradle Wrapper.

On Unix-like platforms such as Linux and Mac OS X:

```
./gradlew clean assemble
```

On Windows:

```
gradlew.bat clean assemble
```

## How to run tests

Before you can run batch application tests, you need to install Asakusa Framewrok runtime environment on your machine.

The first you need to set `ASAKUSA_HOME` environment variable to Asakusa Framework install directory path.

```
export ASAKUSA_HOME=$HOME/asakusafw
```

And you can install with `installAsakusafw` gradle task. This installs Asakusa Framework runtime modules to `ASAKUSA_HOME`.

```
./gradlew installAsakusafw
```

Then you can run any tests.

```
./gradlew test
```

## How to deploy

A deployment archive file `build/asakusafw-example-basic-spark.tar.gz` is generated by run `./gradlew assemble`.
You can put it on your target machine and extract under `ASAKUSA_HOME` directory.

```
mkdir -p "$ASAKUSA_HOME"
cd "$ASAKUSA_HOME"
tar -xf /path/to/asakusafw-example-basic-spark.tar.gz
find "$ASAKUSA_HOME" -name "*.sh" | xargs chmod u+x
```

## How to run application

### Supported platforms

[Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark) is currently tested on some Linux platforms (CentOS, Ubuntu, Amazon Linux).

### Requirements

* Java SE Development Kit >= 1.8
* [Apache Spark](http://spark.apache.org/) 2.0.1

### Setting sample data

This example application reads input data form `target/testing/direcito` on hadoop filesystem by default.
You can use sample data includes `$ASAKUSA_HOME/example-dataset`.

```
hadoop fs -mkdir -p target/testing/directio
hadoop fs -put $ASAKUSA_HOME/example-dataset/master target/testing/directio/master
hadoop fs -put $ASAKUSA_HOME/example-dataset/sales target/testing/directio/sales
```

### Setting environment variables

You need to set `SPARK_CMD` environment variables to `spark-submit` command path of using Spark environment, or set path to `<path/to/spark>/bin` in your `PATH` environment variables.

```
export SPARK_CMD=/opt/spark/bin/spark-submit
```

### Running application

Now you can run application by the following command.

```
$ASAKUSA_HOME/yaess/bin/yaess-batch.sh spark.example.summarizeSales -A date=2011-04-01
```

Then you should see the output files on `$HOME/target/testing/directio/result` directory.

* `category/result.csv`
* `error/${date}.csv`

## Referred Projects
* [Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark)

## Resources
* [Asakusa on Spark Documentation (ja)](http://docs.asakusafw.com/asakusa-on-spark/)
